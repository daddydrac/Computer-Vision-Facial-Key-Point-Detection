{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Filters\n",
    "\n",
    "Using your trained facial keypoint detector, you can now do things like add filters to a person's face, automatically. In this optional notebook, you can play around with adding sunglasses to detected face's in an image by using the keypoints detected around a person's eyes. Checkout the `images/` directory to see what pther .png's have been provided for you to try, too!\n",
    "\n",
    "<img src=\"images/face_filter_ex.png\" width=60% height=60%/>\n",
    "\n",
    "Let's start this process by looking at a sunglasses .png that we'll be working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary resources\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in sunglasses image with cv2 and IMREAD_UNCHANGED\n",
    "sunglasses = cv2.imread('images/sunglasses.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# plot our image\n",
    "plt.imshow(sunglasses)\n",
    "\n",
    "# print out its dimensions\n",
    "print('Image shape: ', sunglasses.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4th dimension\n",
    "\n",
    "You'll note that this image actually has *4 color channels*, not just 3 as your avg RGB image does. This is due to the flag we set `cv2.IMREAD_UNCHANGED`, which tells this to read in another color channel.\n",
    "\n",
    "#### Alpha channel\n",
    "It has the usual red, blue, and green channels any color image has, and the 4th channel respresents the **transparency level of each pixel** in the image; this is often called the **alpha** channel. Here's how the transparency channel works: the lower the value, the more transparent, or see-through, the pixel will become.  The lower bound (completely transparent) is zero here, so any pixels set to 0 will not be seen; these look like white background pixels in the image above, but they are actually totally transparent. \n",
    "\n",
    "This transparent channel allows us to place this rectangular image of sunglasses on an image of a face and still see the face area that is techically covered by the transparentbackground of the sunglasses image!\n",
    "\n",
    "Let's check out the alpha channel of our sunglasses image in the next Python cell. Because many of the pixels in the background of the image have an alpha value of 0, we'll need to explicitly print out non-zero values if we want to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the sunglasses transparency (alpha) channel\n",
    "alpha_channel = sunglasses[:,:,3]\n",
    "print ('The alpha channel looks like this (black pixels = transparent): ')\n",
    "plt.imshow(alpha_channel, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to double check that there are indeed non-zero values\n",
    "# let's find and print out every value greater than zero\n",
    "values = np.where(alpha_channel != 0)\n",
    "print ('The non-zero values of the alpha channel are: ')\n",
    "print (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlaying images\n",
    "\n",
    "This means that when we place this sunglasses image on top of another image, we can use the transparency channel as a filter:\n",
    "\n",
    "* If the pixels are non-transparent (alpha_channel > 0), overlay them on the new image\n",
    "\n",
    "#### Keypoint locations\n",
    "\n",
    "In doing this, it's helpful to understand which keypoint belongs to the eyes, mouth, etc., so in the image below we also print the index of each facial keypoint directly on the image so you can tell which keypoints are for the eyes, eyebrows, etc.,\n",
    "\n",
    "<img src=\"images/landmarks_numbered.jpg\" width=50% height=50%/>\n",
    "\n",
    "It may be useful to use keypoints that correspond to the edges of the face to define the width of the sunglasses, and the locations of the eyes to define the placement.\n",
    "\n",
    "Next, we'll load in an example image. Below, you've been given an image and set of keypoints from the provided training set of data, but you can use your own CNN model to generate keypoints for *any* image of a face (as in Notebook 3) and go through the same overlay process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data if you have not already!\n",
    "# otherwise, you may comment out this cell\n",
    "# -- DO NOT CHANGE THIS CELL -- #\n",
    "!mkdir /data\n",
    "!wget -P /data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n",
    "!unzip -n /data/train-test-data.zip -d /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in training data\n",
    "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\n",
    "\n",
    "# print out some stats about the data\n",
    "print('Number of images: ', key_pts_frame.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function to display keypoints\n",
    "def show_keypoints(image, key_pts):\n",
    "    \"\"\"Show image with keypoints\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a selected image\n",
    "n = 120\n",
    "image_name = key_pts_frame.iloc[n, 0]\n",
    "image = mpimg.imread(os.path.join('/data/training/', image_name))\n",
    "key_pts = key_pts_frame.iloc[n, 1:].as_matrix()\n",
    "key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "\n",
    "print('Image name: ', image_name)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "show_keypoints(image, key_pts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll see an example of placing sunglasses on the person in the loaded image.\n",
    "\n",
    "Note that the keypoints are numbered off-by-one in the numbered image above, and so `key_pts[0,:]` corresponds to the first point (1) in the labelled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sunglasses on top of the image in the appropriate place\n",
    "\n",
    "# copy of the face image for overlay\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "# top-left location for sunglasses to go\n",
    "# 17 = edge of left eyebrow\n",
    "x = int(key_pts[17, 0])\n",
    "y = int(key_pts[17, 1])\n",
    "\n",
    "# height and width of sunglasses\n",
    "# h = length of nose\n",
    "h = int(abs(key_pts[27,1] - key_pts[34,1]))\n",
    "# w = left to right eyebrow edges\n",
    "w = int(abs(key_pts[17,0] - key_pts[26,0]))\n",
    "\n",
    "# read in sunglasses\n",
    "sunglasses = cv2.imread('images/sunglasses.png', cv2.IMREAD_UNCHANGED)\n",
    "# resize sunglasses\n",
    "new_sunglasses =  cv2.resize(sunglasses, (w, h), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# get region of interest on the face to change\n",
    "roi_color = image_copy[y:y+h,x:x+w]\n",
    "\n",
    "# find all non-transparent pts\n",
    "ind = np.argwhere(new_sunglasses[:,:,3] > 0)\n",
    "\n",
    "# for each non-transparent point, replace the original image pixel with that of the new_sunglasses\n",
    "for i in range(3):\n",
    "    roi_color[ind[:,0],ind[:,1],i] = new_sunglasses[ind[:,0],ind[:,1],i]    \n",
    "# set the area of the image to the changed region with sunglasses\n",
    "image_copy[y:y+h,x:x+w] = roi_color\n",
    "\n",
    "\n",
    "# display the result!\n",
    "plt.imshow(image_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Further steps\n",
    "\n",
    "Look in the `images/` directory to see other available .png's for overlay! Also, you may notice that the overlay of the sunglasses is not entirely perfect; you're encouraged to play around with the scale of the width and height of the glasses and investigate how to perform [image rotation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html) in OpenCV so as to match an overlay with any facial pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
